---
title: 'Spider-Wo-Men: with great power comes great responsibility'
author: "Alba Proficuo_Erica Ravarelli"
date: '2022-02-22'
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(httr)
library(curl)
library(rvest)
library(knitr)
```

# First Point

1)	Inspect the robots.txt. Unusual, right? What will you do?

Inspecting the page with /robots.txt brings us to a 404 not found page.  This means that there is no robots.txt: the web master doesn't provide any restriction to scrapigin activities. Therefore, we decided to proceed with the following tasks of the exercise.

# Second Point

2)	Check out the following link: http://www.beppegrillo.it/un-mare-diplastica-ci-sommergera/. Download it using rcurl: :getURL() to download the page while informing the webmaster about your browser details and providing your email.

We created a function `download_politely(from_url,to_html)` to download politely the web page:

```{r}
url <- "https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/"

require(httr)


download_politely <- function(from_url, to_html, my_email, my_agent = R.Version()$version.string) {
  
  require(httr)
  
  stopifnot(is.character(from_url))
  stopifnot(is.character(to_html))
  stopifnot(is.character(my_email))
  
  blog <- httr::GET(url = from_url, 
                    add_headers(
                      From = my_email, 
                      `User-Agent` = R.Version()$version.string
                    )
  )
  if (httr::http_status(blog)$message == "Success: (200) OK") {
    bin <- content(blog, as = "raw")
    writeBin(object = bin, con = to_html)
  } else {
    cat("Houston, we have a problem!")
  }
}

download_politely(from_url = url, 
                  to_html = here::here("blog_polite.html"), 
                  my_email = "ravarellierica@gmail.com")



```

# Third point 

3)	Create a data frame with all the HTML links in the page. You can use rvest:: or check out the XML::getHTMLLinks function. Then, use a regex to keep only those links that re-direct to other posts of the beppegrillo.it blog (so remove all other links)

We did the parsing of the downloaded page, extracting only nodes corresponding to links. Since the vector we obtained contaned also missing values, we applied a regular expression in order to get only strings beginning with "https"

```{r}
blog_links <- rvest::read_html(x = url) %>% 
  html_elements(css="a")%>% 
  html_attr("href")

all_links <- str_subset(blog_links,  pattern = "^https?.*")
```

We got only Grillo links appling another regular expression, then we removed duplicates and we created a data frame.

```{r}
onlyGrillo_links <- str_subset(blog_links, pattern="^https?://beppegrillo.it.*")

without_dubles <- unique(onlyGrillo_links)

Grillo_links <- tibble(
          x1=without_dubles,
          x2=1:29)
kable((Grillo_links), col.names = c("link", "number"), align="l")
```

# Fourth point

4)	Check out the following link: http://www.beppegrillo.it/category/archivio/2016/. It contains the entire blog for 2016. There are 47 pages of entries. Scrape all the posts for 2016 following this strategy: 
a. For each of the 47 pages, get all the links and place them into a list (or character vector). Tip: see how the URL changes to build the loop! 
b. For each single linked blog post, download the page as a file and sys .sleep () a little. 
c. For each downloaded page, scrape the main text. Ask yourself what happens if a page contains no text. 

We downloaded the page, then we built the full list of links to each page using string concatenate. We downloaded each single page, storing them in a new folder called "links 2016". 

```{r}

download_politely <- function(from_url, to_html, my_email, my_agent = R.Version()$version.string) {require(httr)
  stopifnot(is.character(from_url))
  stopifnot(is.character(to_html))
  stopifnot(is.character(my_email))
  blog <- httr::GET(url = from_url, 
                    add_headers(
                      From = my_email, 
                      `User-Agent` = R.Version()$version.string
                    )
  )
  if (httr::http_status(blog)$message == "Success: (200) OK") {
    bin <- content(blog, as = "raw")
    writeBin(object = bin, con = to_html)
  } else {
    cat("Houston, we have a problem!")
  }
}

require(stringr)
links <- str_c("https://beppegrillo.it/category/archivio/2016/page/", 1:47)
dir.create("links 2016")   


for (i in seq_along(links)) {
  cat(i, " ")
  download_politely(from_url = links[i], 
                    to_html = here::here("links 2016", str_c("page_",i,".html")), 
                    my_email = "albaproficuo@icloud.com")
  
  Sys.sleep(0.5)
}
```

We scraped the main text from each page with a loop. We got the list of pages that we used to scrape the text, then we created an empty container where to place it. We used the selector gadget in order to understand which css selector was the right one. We obtained a list from which we removed duplicates, obtaining another list (`without_duplicates`) with the main text as a character vector.

```{r}
to_scrape <- list.files(here::here("links 2016"), full.names = TRUE)   
main_text <- vector(mode = "list", length = length(to_scrape))    

for (i in seq_along(main_text)){
  main_text[[i]] <- read_html(to_scrape[i]) %>% 
    html_elements(css = ".td_module_10 .td-module-title") %>% 
    html_text(trim = TRUE)
}

without_duplicates <- unique(main_text)
kable(without_duplicates[[1]], col.names="main text page 1")
kable(without_duplicates[[2]], col.names="main text page 2")
kable(without_duplicates[[3]], col.names="main text page 3")
```


If in a page i there is no text, the CSS selector doesn't select anything and there is a missing value (NA) in the list main_text.

# Fifth point

5)	Check out the RCrawler package and its accompanying paper. What does it mean to “crawl”? and what is it a “web spider”? How is this different from a scraper you have built at point 5? Inspect the package documentation and sketch how you could build a spider scraper: which function(s) should you use? With which arguments? Don't do it, just sketch and explain.

The **RCrawler** is a package used for web crawling, downloading webpages and content scraping.
Basically this package can crawl web sites and extracts their content using various 
techniques, from a given URL RCrawler can automatically crawl and parse all URLs in that 
domain, and extract specific content from these URLs that matches the user criteria.
Among other packages like "scrapeR" and "Rvest", or basic web toolkits like "XML", "Selectr",
"Httr" and "RCurl", RCrawler is the only one that can crawl. 

**Crawl** is a method of finding web links originating from one URL or a list of URLs. Crawling 
can be implemented with data extraction either as two separate consecutive tasks or as 
simultaneous tasks. 

**Web spiders** are programs that automatically browse and download web pages by following 
hyperlinks in a methodical and automated manner, therefore are programs that can 
crawl. There are various types of web crawlers:
  1. "**universal crawlers**" are intended to crawl and index all web pages without minding 
      their content;
  2. "**preferential crawlers**", are more targeted towards a specific topic. 
Web crawlers are known primarily for supporting the actions of search engines and collecting
web pages, but are also used in web page content mining applications. 

We can say that web crawling is different from the scraping we did in the previous point 
because while we extracted the data from a specific website with a known domain and URL 
(in this case "https://beppegrillo.it/category/archivio/2016/"), web spiders are 
programes that help finding or discovering unknown URLs, domains or links on the web. 
Moreover, Rcrawler allows us to implement crawling and data extraction simultaneously. 
Source: RCrawler: Samlim, K., Fakir, M., _An R package for parallel web crawling and scraping_.


Hypothesizing how to build a spider scraper:
Rcrawler(Website, RequestsDelay = 0.5, Obeyrobots = TRUE, Useragent = "Chrome version", 
        crawlUrlfilter = "^https?://beppegrillo.it.*") 

Website: "https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/" (the URL of the website to crawl)
everything else default, exept:
RequestsDelay: 0.5 (to avoid being banned beacuse of the too many requests)
Obeyrobots: TRUE, to obey to rules set by the webmaster concerning content scraping/crawling
Useragent: "Chrome version" 
crawlUrlfilter: "^https?://beppegrillo.it.*" if we want to filter URLs to be crawled, 
obtaining only links that re-direct to other posts of the beppegrillo.it blog

# Individual constributions

- number of commits per person: Alba Proficuo_ ..., Erica Ravarelli_...; 
- number of issues opened: 2;
- number of pull requests opened: 1;
- number of pull requests accepted and merged: 1;
- number of new branches opened: 1.

Individual tasks:

Erica Ravarelli: Inspect robots.txt, download the link, https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/, inform the webmaster on our identity, do dataframe with all HTML links, select the links that redirect to the post of https://beppegrillo.it site with regular expressions, scrape the text from the downloaded pages, sketch and design of the spider scraper. 

Alba Proficuo: Inspect robots.txt, https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/, inform the webmaster on our identity, put all 47 links to the pages in a list, download the page of every link, scrape post from 2016 from the site https://beppegrillo.it/category/archivio/2016/, definition of 'crawl' and 'web spider', differences between crawling and the scraping we did in the last task, 



