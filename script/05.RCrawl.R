The RCrawler is a package used for web crawling, downloading webpages and content scraping.
Basically this package can crawl web sites and extracts their content using various 
techniques, from a given URL RCrawler can automatically crawl and parse all URLs in that 
domain, and extract specific content from these URLs that matches the user criteria.
Among other packages like "scrapeR" and "Rvest", or basic web toolkits like "XML", "Selectr",
"Httr" and "RCurl", RCrawler is the only one that can crawl. 

Crawl is a method of finding web links originating from one URL or a list of URLs. Crawling 
can be implemented with data extraction either as two separate consecutive tasks or as 
simultaneous tasks. 

Web spiders are programs that automatically browse and download web pages by following 
hyperlinks in a methodical and automated manner, therefore are programm that can 
crawl. There are various types of web crawlers:
  1. "universal crawlers" are intended to crawl and index all web pages without minding 
      their content;
  2. "preferential crawlers", are more targeted towards a specific topic. 
Web crawlers are known primarily for supporting the actions of search engines and collecting
web pages, but are also used in web page content mining applications. 

We can say that web crawling is different from the scraping we did in the previous point 
because while we extracted the data from a specific website with a known domanin and URL 
( in this case "https://beppegrillo.it/category/archivio/2016/"), web spiders are 
programmes that help finding or discovering unknown URLs, domains or links on the web. 
Moreover, Rcrawler allows us to implement crawling and data extraction simultaneously. 
